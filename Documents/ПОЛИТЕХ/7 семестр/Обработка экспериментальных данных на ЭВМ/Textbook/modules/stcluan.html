<html>

<head>
<title>Кластерный анализ</title>
</head>

<body BACKGROUND="../tile1.gif">
<font SIZE="5" COLOR="AA0000"><b>

<p align="center">Кластерный анализ</b></font> </p>

<hr SIZE="1">

<p><a NAME="index"></a> 

<ul>
  <li><a HREF="stcluan.html#general">Основная цель</a> </li>
  <li><a HREF="stcluan.html#statistical">Проверка статистической
    значимости</a> </li>
  <li><a HREF="stcluan.html#area">Области применения</a> </li>
  <li><a HREF="stcluan.html#joining">Объединение (древовидная
    кластеризация)</a> <ul>
      <li><a HREF="stcluan.html#h">Иерархическое дерево</a> </li>
      <li><a HREF="stcluan.html#d">Меры расстояния</a> </li>
      <li><a HREF="stcluan.html#a">Правила объединения или связи</a> </li>
    </ul>
  </li>
  <li><a HREF="stcluan.html#two">Двувходовое объединение</a> <ul>
      <li><a HREF="stcluan.html#twointro">Вводный обзор</a> </li>
      <li><a HREF="stcluan.html#twotwo">Двувходовое объединение</a> </li>
    </ul>
  </li>
  <li><a HREF="stcluan.html#k">Метод K средних</a> <ul>
      <li><a HREF="stcluan.html#kexample">Пример</a> </li>
      <li><a HREF="stcluan.html#kcomputations">Вычисления</a> </li>
      <li><a HREF="stcluan.html#kinterpret">Интерпретация результатов</a> </li>
    </ul>
  </li>
</ul>

<hr SIZE="1">

<p><a NAME="general"></a></p>

<p><font SIZE="5" COLOR="navy">Основная цель </font></p>

<p>Термин <i>кластерный анализ </i>(впервые ввел Tryon,
1939) в действительности включает в себя набор
различных <a href="../glossary/gloss_a.html#Algorithm">алгоритмов</a>
классификации. Общий вопрос, задаваемый
исследователями во многих областях, состоит в
том, как <i>организовать</i> наблюдаемые данные в
наглядные структуры, т.е. развернуть таксономии.
Например, биологи ставят цель разбить животных
на различные виды, чтобы содержательно описать
различия между ними. В соответствии с
современной системой, принятой в биологии,
человек принадлежит к приматам, млекопитающим,
амниотам, позвоночным и животным. Заметьте, что в
этой классификации, чем выше уровень агрегации,
тем меньше сходства между членами в
соответствующем классе. Человек имеет больше
сходства с другими приматами (т.е. с обезьянами),
чем с &quot;отдаленными&quot; членами семейства
млекопитающих (например, собаками) и т.д. В
последующих разделах будут рассмотрены общие
методы кластерного анализа, см. <a href="stcluan.html#joining"><i>Объединение
(древовидная кластеризация)</i></a>, <a href="stcluan.html#two"><i>Двувходовое
объединение</i></a> и <a href="stcluan.html#k"><i>Метод K средних</i></a>.</p>

<p><a NAME="statistical"></a><font SIZE="5" COLOR="navy"> Проверка
статистической значимости </font></p>

<p>Заметим, что предыдущие рассуждения ссылаются
на алгоритмы кластеризации, но ничего не
упоминают о проверке статистической значимости.
Фактически, кластерный анализ является не
столько обычным статистическим методом, сколько
&quot;набором&quot; различных <a href="../glossary/gloss_a.html#Algorithm">алгоритмов</a>
&quot;распределения объектов по кластерам&quot;.
Существует точка зрения, что в отличие от многих
других статистических процедур, методы
кластерного анализа используются в большинстве
случаев тогда, когда вы не имеете каких-либо
априорных гипотез относительно классов, но все
еще находитесь в описательной стадии
исследования. Следует понимать, что кластерный
анализ определяет &quot;наиболее возможно значимое
решение&quot;. Поэтому проверка статистической
значимости в действительности здесь
неприменима, даже в случаях, когда известны
p-уровни (как, например, в <a href="stcluan.html#k">методе K
средних</a>). </p>

<p><a NAME="area"></a><font SIZE="5" COLOR="navy"> Области применения</font></p>

<p>Техника кластеризации применяется в самых
разнообразных областях. Хартиган (Hartigan, 1975) дал
прекрасный обзор многих опубликованных
исследований, содержащих результаты, полученные
методами кластерного анализа. Например, в
области медицины кластеризация заболеваний,
лечения заболеваний или симптомов заболеваний
приводит к широко используемым таксономиям. В
области психиатрии правильная диагностика
кластеров симптомов, таких как паранойя,
шизофрения и т.д., является решающей для успешной
терапии. В археологии с помощью кластерного
анализа исследователи пытаются установить
таксономии каменных орудий, похоронных объектов
и т.д. Известны широкие применения кластерного
анализа в маркетинговых исследованиях. В общем,
всякий раз, когда необходимо классифицировать
&quot;горы&quot; информации к пригодным для
дальнейшей обработки группам, кластерный анализ
оказывается весьма полезным и эффективным. </p>

<table ALIGN="RIGHT">
  <tr>
    <td><font SIZE="1"><a HREF="stcluan.html#index">В начало</a></font> </td>
  </tr>
</table>

<p><br>
<br CLEAR="RIGHT">
</p>

<hr SIZE="1">

<p><a NAME="joining"></a> <font SIZE="5" COLOR="navy">Объединение
(древовидная кластеризация) </font>

<ul>
  <li><a HREF="stcluan.html#h">Иерархическое дерево</a> </li>
  <li><a HREF="stcluan.html#d">Меры расстояния</a> </li>
  <li><a HREF="stcluan.html#a">Правила объединения или связи</a> </li>
</ul>
<font SIZE="4" COLOR="navy">

<p>Общая логика </font></p>

<p>Приведенный в разделе <a href="stcluan.html#general">Основная
цель</a> пример поясняет цель алгоритма
объединения (<i>древовидной кластеризации</i>).
Назначение этого <a href="../glossary/gloss_a.html#Algorithm">алгоритма</a>
состоит в объединении объектов (например,
животных) в достаточно большие кластеры,
используя некоторую меру сходства или
расстояние между объектами. Типичным
результатом такой кластеризации является
иерархическое дерево.</p>

<p><a NAME="h"></a><font SIZE="4" COLOR="navy"> Иерархическое дерево </font></p>

<p>Рассмотрим <i>горизонтальную древовидную
диаграмму</i>. Диаграмма начинается с каждого
объекта в классе (в левой части диаграммы). Теперь
представим себе, что постепенно (очень малыми
шагами) вы &quot;ослабляете&quot; ваш критерий о том,
какие объекты являются уникальными, а какие нет.
Другими словами, вы понижаете порог, относящийся
к решению об объединении двух или более объектов
в один кластер. </p>

<p><img BORDER="0" SRC="../popups/popup12.gif" alt="Древовидная диаграмма" WIDTH="306" HEIGHT="218"> </p>

<p>В результате, вы <i>связываете </i>вместе всё
большее и большее число объектов и агрегируете (<i>объединяете</i>)
все больше и больше кластеров, состоящих из все
сильнее различающихся элементов. Окончательно,
на последнем шаге все объекты объединяются
вместе. На этих диаграммах горизонтальные оси
представляют расстояние объединения (в <i>вертикальных
древовидных диаграммах </i>вертикальные оси
представляют расстояние объединения). Так, для
каждого узла в графе (там, где формируется новый
кластер) вы можете видеть величину расстояния,
для которого соответствующие элементы
связываются в новый единственный кластер. Когда
данные имеют ясную &quot;структуру&quot; в терминах
кластеров объектов, сходных между собой, тогда
эта структура, скорее всего, должна быть отражена
в иерархическом дереве различными ветвями. В
результате успешного анализа методом
объединения появляется возможность обнаружить
кластеры (ветви) и интерпретировать их.</p>

<p><a NAME="d"></a><font SIZE="4" COLOR="navy"> Меры расстояния</font></p>

<p>Объединение или метод древовидной
кластеризации используется при формировании
кластеров несходства или расстояния между
объектами. Эти расстояния могут определяться в
одномерном или многомерном пространстве.
Например, если вы должны кластеризовать типы еды
в кафе, то можете принять во внимание количество
содержащихся в ней калорий, цену, субъективную
оценку вкуса и т.д. Наиболее прямой путь
вычисления расстояний между объектами в
многомерном пространстве состоит в вычислении
евклидовых расстояний. Если вы имеете двух- или
трёхмерное пространство, то эта мера является
реальным геометрическим расстоянием между
объектами в пространстве (как будто расстояния
между объектами измерены рулеткой). Однако
алгоритм объединения не &quot;заботится&quot; о том,
являются ли &quot;предоставленные&quot; для этого
расстояния настоящими или некоторыми другими
производными мерами расстояния, что более
значимо для исследователя; и задачей
исследователей является подобрать правильный
метод для специфических применений. </p>

<p><b>Евклидово расстояние. </b>Это, по-видимому,
наиболее общий тип расстояния. Оно попросту
является геометрическим расстоянием в
многомерном пространстве и вычисляется
следующим образом: </p>

<p><font color="blue">расстояние(x,y) = {<img src="../graphics/sigmablu.gif" align="absmiddle" WIDTH="12" HEIGHT="19"><sub>i</sub> (x<sub>i</sub> - y<sub>i</sub>)<sup>2</sup> }<sup>1/2</sup>
</font></p>

<p>Заметим, что евклидово расстояние (и его
квадрат) вычисляется по исходным, а не по
стандартизованным данным. Это обычный способ его
вычисления, который имеет определенные
преимущества (например, расстояние между двумя
объектами не изменяется при введении в анализ
нового объекта, который может оказаться
выбросом). Тем не менее, на расстояния могут
сильно влиять различия между осями, по
координатам которых вычисляются эти расстояния.
К примеру, если одна из осей измерена в
сантиметрах, а вы потом переведете ее в
миллиметры (умножая значения на 10), то
окончательное евклидово расстояние (или квадрат
евклидова расстояния), вычисляемое по
координатам, сильно изменится, и, как следствие,
результаты кластерного анализа могут сильно
отличаться от предыдущих. </p>

<p><b>Квадрат евклидова расстояния.</b> Иногда может
возникнуть желание возвести в квадрат
стандартное евклидово расстояние, чтобы придать
большие веса более отдаленным друг от друга
объектам. Это расстояние вычисляется следующим
образом (см. также замечания в предыдущем пункте):
</p>

<p><font color="blue">расстояние(x,y) = <img src="../graphics/sigmablu.gif" align="absmiddle" WIDTH="12" HEIGHT="19"><sub>i</sub> (x<sub>i</sub> - y<sub>i</sub>)<sup>2</sup> </font></p>

<p><b>Расстояние городских кварталов
(манхэттенское расстояние). </b>Это расстояние
является просто средним разностей по
координатам. В большинстве случаев эта мера
расстояния приводит к таким же результатам, как и
для обычного расстояния Евклида. Однако отметим,
что для этой меры влияние отдельных больших
разностей (выбросов) уменьшается (так как они не
возводятся в квадрат). Манхэттенское расстояние
вычисляется по формуле: </p>

<p><font color="blue">расстояние(x,y) = <img src="../graphics/sigmablu.gif" align="absmiddle" WIDTH="12" HEIGHT="19"><sub>i</sub> |x<sub>i</sub> - y<sub>i</sub>| </font></p>

<p><b>Расстояние Чебышева. </b>Это расстояние может
оказаться полезным, когда желают определить два
объекта как &quot;различные&quot;, если они
различаются по какой-либо одной координате
(каким-либо одним измерением). Расстояние
Чебышева вычисляется по формуле: </p>

<p><font color="blue">расстояние(x,y) = Максимум|x<sub>i</sub> - y<sub>i</sub>|
</font></p>

<p><b>Степенное расстояние. </b>Иногда желают
прогрессивно увеличить или уменьшить вес,
относящийся к размерности, для которой
соответствующие объекты сильно отличаются. Это
может быть достигнуто с использованием<i>
степенного расстояния</i>. Степенное расстояние
вычисляется по формуле: </p>

<p><font color="blue">расстояние(x,y) = (<img src="../graphics/sigmablu.gif" align="absmiddle" WIDTH="12" HEIGHT="19"><sub>i</sub> |x<sub>i</sub> - y<sub>i</sub>|<sup>p</sup>)<sup>1/r</sup> </font></p>

<p>где <font color="blue"><i>r</i></font> и <i><font color="blue">p</font> - </i>параметры,
определяемые пользователем. Несколько примеров
вычислений могут показать, как &quot;работает&quot;
эта мера. Параметр <font color="blue"><i>p</i></font>
ответственен за постепенное взвешивание
разностей по отдельным координатам, параметр <font color="blue"><i>r</i></font> ответственен за прогрессивное
взвешивание больших расстояний между объектами.
Если оба параметра - <font color="blue"><i>r</i></font> и <font color="blue"><i>p</i></font>, равны двум, то это расстояние
совпадает с расстоянием Евклида. </p>

<p><b>Процент несогласия. </b>Эта мера используется в
тех случаях, когда данные являются
категориальными. Это расстояние вычисляется по
формуле: </p>

<p><font color="blue">расстояние(x,y) = (Количество x<sub>i</sub> <img src="../graphics/netblue.gif" align="absmiddle" WIDTH="11" HEIGHT="12"> y<sub>i</sub>)/ i </font></p>

<p><a NAME="a"></a><font SIZE="4" COLOR="navy"> Правила объединения
или связи </font></p>

<p>На первом шаге, когда каждый объект
представляет собой отдельный кластер,
расстояния между этими объектами определяются
выбранной мерой. Однако когда связываются вместе
несколько объектов, возникает вопрос, как
следует определить расстояния между кластерами?
Другими словами, необходимо правило объединения
или связи для двух кластеров. Здесь имеются
различные возможности: например, вы можете
связать два кластера вместе, когда <i>любые</i> два
объекта в двух кластерах ближе друг к другу, чем
соответствующее расстояние связи. Другими
словами, вы используете &quot;правило ближайшего
соседа&quot; для определения расстояния между
кластерами; этот метод называется методом <i>одиночной
связи</i>. Это правило строит &quot;волокнистые&quot;
кластеры, т.е. кластеры, &quot;сцепленные вместе&quot;
только отдельными элементами, случайно
оказавшимися ближе остальных друг к другу. Как
альтернативу вы можете использовать соседей в
кластерах, которые находятся дальше всех
остальных пар объектов друг от друга. Этот метод
называется метод<i> полной связи</i>. Существует
также множество других методов объединения
кластеров, подобных тем, что были рассмотрены. </p>

<p><b>Одиночная связь (метод ближайшего соседа). </b>Как
было описано выше, в этом методе расстояние между
двумя кластерами определяется расстоянием между
двумя наиболее близкими объектами (ближайшими
соседями) в различных кластерах. Это правило
должно, в известном смысле, <i>нанизывать </i>объекты
вместе для формирования кластеров, и
результирующие кластеры имеют тенденцию быть
представленными длинными &quot;цепочками&quot;. </p>

<p><b>Полная связь (метод наиболее удаленных
соседей). </b>В этом методе расстояния между
кластерами определяются наибольшим расстоянием
между любыми двумя объектами в различных
кластерах (т.е. &quot;наиболее удаленными
соседями&quot;). Этот метод обычно работает очень
хорошо, когда объекты происходят на самом деле из
реально различных &quot;рощ&quot;. Если же кластеры
имеют в некотором роде удлиненную форму или их
естественный тип является &quot;цепочечным&quot;, то
этот метод непригоден. </p>

<p><b>Невзвешенное попарное среднее. </b>В этом
методе расстояние между двумя различными
кластерами вычисляется как среднее расстояние
между всеми парами объектов в них. Метод
эффективен, когда объекты в действительности
формируют различные &quot;рощи&quot;, однако он
работает одинаково хорошо и в случаях
протяженных (&quot;цепочного&quot; типа) кластеров.
Отметим, что в своей книге Снит и Сокэл (Sneath, Sokal,
1973) вводят аббревиатуру <i>UPGMA</i> для ссылки на этот
метод, как на <i>метод невзвешенного попарного
арифметического среднего</i> - <i>unweighted pair-group method
using arithmetic averages</i>. </p>

<p><b>Взвешенное попарное среднее. </b>Метод
идентичен методу <i>невзвешенного попарного
среднего</i>, за исключением того, что при
вычислениях размер соответствующих кластеров
(т.е. число объектов, содержащихся в них)
используется в качестве весового коэффициента.
Поэтому предлагаемый метод должен быть
использован (скорее даже, чем предыдущий), когда
предполагаются неравные размеры кластеров. В
книге Снита и Сокэла (Sneath, Sokal, 1973) вводится
аббревиатура <i>WPGMA</i> для ссылки на этот метод, как
на <i>метод взвешенного попарного
арифметического среднего -</i> <i>weighted pair-group method using
arithmetic averages</i>. </p>

<p><b>Невзвешенный центроидный метод. </b>В этом
методе расстояние между двумя кластерами
определяется как расстояние между их центрами
тяжести. Снит и Сокэл (Sneath and Sokal (1973)) используют
аббревиатуру <i>UPGMC</i> для ссылки на этот метод, как
на <i>метод невзвешенного попарного центроидного
усреднения</i> - <i>unweighted pair-group method using the centroid average</i>.
</p>

<p><b>Взвешенный центроидный метод (медиана). </b>тот
метод идентичен предыдущему, за исключением
того, что при вычислениях используются веса для
учёта разницы между размерами кластеров (т.е.
числами объектов в них). Поэтому, если имеются
(или подозреваются) значительные отличия в
размерах кластеров, этот метод оказывается
предпочтительнее предыдущего. Снит и Сокэл (Sneath,
Sokal 1973) использовали аббревиатуру <i>WPGMC </i>для
ссылок на него, как на <i>метод невзвешенного
попарного центроидного усреднения</i> - <i>weighted
pair-group method using the centroid average</i>. </p>

<p><b>Метод Варда. </b>Этот метод отличается от всех
других методов, поскольку он использует методы
дисперсионного анализа для оценки расстояний
между кластерами. Метод минимизирует сумму
квадратов (SS) для любых двух (гипотетических)
кластеров, которые могут быть сформированы на
каждом шаге. Подробности можно найти в работе
Варда (Ward, 1963). В целом метод представляется очень
эффективным, однако он стремится создавать
кластеры малого размера. </p>

<p>Для обзора других методов кластеризации, см. <a href="stcluan.html#two">Двухвходовое объединение</a> и <a href="stcluan.html#k">Метод K средних</a>. </p>

<table ALIGN="RIGHT">
  <tr>
    <td><font SIZE="1"><a HREF="stcluan.html#index">В начало</a></font> </td>
  </tr>
</table>

<p><br>
<br CLEAR="RIGHT">
</p>

<hr SIZE="1">

<p><a NAME="two"></a> <font SIZE="5" COLOR="navy">Двувходовое
объединение</font> 

<ul>
  <li><a HREF="stcluan.html#twointro">Вводный обзор</a> </li>
  <li><a HREF="stcluan.html#twotwo">Двувходовое объединение</a> </li>
</ul>

<p><a NAME="twointro"></a></p>
<font SIZE="4" COLOR="navy">

<p>Вводный обзор </font></p>

<p>Ранее этот метод обсуждался в терминах
&quot;объектов&quot;, которые должны быть
кластеризованы (см. <a href="stcluan.html#joining">Объединение
(древовидная кластеризация)</a>). Во всех других
видах анализа интересующий исследователя вопрос
обычно выражается в терминах наблюдений или
переменных. Оказывается, что кластеризация, как
по наблюдениям, так и по переменным может
привести к достаточно интересным результатам.
Например, представьте, что медицинский
исследователь собирает данные о различных
характеристиках (переменные) состояний
пациентов (наблюдений), страдающих сердечными
заболеваниями. Исследователь может захотеть
кластеризовать наблюдения (пациентов) для
определения кластеров пациентов со сходными
симптомами. В то же самое время исследователь
может захотеть кластеризовать переменные для
определения кластеров переменных, которые
связаны со сходным физическим состоянием.</p>

<p><a NAME="twotwo"></a><font SIZE="4" COLOR="navy"> Двувходовое
объединение </font></p>

<p>После этого обсуждения, относящегося к тому,
кластеризовать наблюдения или переменные, можно
задать вопрос, а почему бы не проводить
кластеризацию в обоих направлениях? Модуль <i>Кластерный
анализ </i>содержит эффективную двувходовую
процедуру объединения, позволяющую сделать
именно это. Однако двувходовое объединение
используется (относительно редко) в
обстоятельствах, когда ожидается, что и
наблюдения и переменные одновременно вносят
вклад в обнаружение осмысленных кластеров. </p>

<p><img BORDER="0" SRC="../popups/popup13.gif" alt="График" WIDTH="291" HEIGHT="213"> </p>

<p>Так, возвращаясь к предыдущему примеру, можно
предположить, что медицинскому исследователю
требуется выделить кластеры пациентов, сходных
по отношению к определенным кластерам
характеристик физического состояния. Трудность
с интерпретацией полученных результатов
возникает вследствие того, что сходства между
различными кластерами могут происходить из (или
быть причиной) некоторого различия подмножеств
переменных. Поэтому получающиеся кластеры
являются по своей природе неоднородными.
Возможно это кажется вначале немного туманным; в
самом деле, в сравнении с другими описанными
методами кластерного анализа (см. <a href="stcluan.html#joining">Объединение (древовидная
кластеризация)</a> и <a href="stcluan.html#k">Метод K средних</a>),
двувходовое объединение является, вероятно,
наименее часто используемым методом. Однако
некоторые исследователи полагают, что он
предлагает мощное средство разведочного анализа
данных (за более подробной информацией вы можете
обратиться к описанию этого метода у Хартигана
(Hartigan, 1975)). </p>

<table ALIGN="RIGHT">
  <tr>
    <td><font SIZE="1"><a HREF="stcluan.html#index">В начало</a></font> </td>
  </tr>
</table>

<p><br>
<br CLEAR="RIGHT">
</p>

<hr SIZE="1">

<p><a NAME="k"></a></p>
<font SIZE="5" COLOR="navy">

<p>Метод K средних </font>

<ul>
  <li><a HREF="stcluan.html#kexample">Пример</a> </li>
  <li><a HREF="stcluan.html#kcomputations">Вычисления</a> </li>
  <li><a HREF="stcluan.html#kinterpret">Интерпретация результатов</a> </li>
</ul>
<font SIZE="4" COLOR="navy">

<p>Общая логика </font></p>

<p>Этот метод кластеризации существенно
отличается от таких агломеративных методов, как <a href="stcluan.html#joining">Объединение (древовидная
кластеризация)</a> и <a href="stcluan.html#two">Двувходовое
объединение</a>. Предположим, вы уже имеете
гипотезы относительно числа кластеров (по
наблюдениям или по переменным). Вы можете указать
системе образовать ровно три кластера так, чтобы
они были настолько различны, насколько это
возможно. Это именно тот тип задач, которые
решает <a href="../glossary/gloss_a.html#Algorithm">алгоритм</a>
метода K средних. В общем случае метод K средних
строит ровно K различных кластеров,
расположенных на возможно больших расстояниях
друг от друга.</p>

<p><a NAME="kexample"></a><font SIZE="4" COLOR="navy">Пример </font></p>

<p>В примере с физическим состоянием (см. <i><a href="stcluan.html#two">Двувходовое объединение</a></i>),
медицинский исследователь может иметь
&quot;подозрение&quot; из своего клинического опыта,
что его пациенты в основном попадают в три
различные категории. Далее он может захотеть
узнать, может ли его интуиция быть подтверждена
численно, то есть, в самом ли деле кластерный
анализ<i> </i>K средних даст три кластера пациентов,
как ожидалось? Если это так, то средние различных
мер физических параметров для каждого кластера
будут давать количественный способ
представления гипотез исследователя (например,
пациенты в кластере 1 имеют высокий параметр 1,
меньший параметр 2 и т.д.).</p>

<p><a NAME="kcomputations"></a><font SIZE="4" COLOR="navy"> Вычисления </font></p>

<p>С вычислительной точки зрения вы можете
рассматривать этот метод, как дисперсионный
анализ (см. <em><a HREF="stanman.html">Дисперсионный анализ</a></em>)
&quot;наоборот&quot;. Программа начинает с K случайно
выбранных кластеров, а затем изменяет
принадлежность объектов к ним, чтобы: (1) -
минимизировать изменчивость <i>внутри</i>
кластеров, и (2) - максимизировать изменчивость <i>между</i>
кластерами. Данный способ аналогичен методу
&quot;дисперсионный анализ (ANOVA) наоборот&quot; в том
смысле, что критерий значимости в дисперсионном
анализе сравнивает межгрупповую изменчивость с
внутригрупповой при проверке гипотезы о том, что
средние в группах отличаются друг от друга. В
кластеризации методом K средних программа
перемещает объекты (т.е. наблюдения) из одних
групп (кластеров) в другие для того, чтобы
получить наиболее значимый результат при
проведении дисперсионного анализа (ANOVA).</p>

<p><a NAME="kinterpret"></a><font SIZE="4" COLOR="navy"> Интерпретация
результатов </font></p>

<p>Обычно, когда результаты кластерного анализа
методом K средних получены, можно рассчитать
средние для каждого кластера по каждому
измерению, чтобы оценить, насколько кластеры
различаются друг от друга. В идеале вы должны
получить сильно различающиеся средние для
большинства, если не для всех измерений,
используемых в анализе. Значения <i>F</i>-статистики,
полученные для каждого измерения, являются
другим индикатором того, насколько хорошо
соответствующее измерение дискриминирует
кластеры.</p>

<p><br>
<br>
</p>

<hr SIZE="1">

<p align="center"><br>
<img SRC="../stathoms.jpg" ALIGN="LEFT" WIDTH="151" HEIGHT="41"> <br CLEAR="ALL">
<font SIZE="1">(c) Copyright StatSoft, Inc., 1984-2001<br>
STATISTICA является торговой маркой StatSoft, Inc. </font></p>

<hr SIZE="1">
</body>
</html>

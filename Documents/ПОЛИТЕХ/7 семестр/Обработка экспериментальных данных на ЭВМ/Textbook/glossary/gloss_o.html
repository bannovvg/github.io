<html>

<head>
<title>Электронный учебник - Словарь О</title>
</head>

<body BACKGROUND="../tile1.gif">

<p><a NAME="Generalization"><font SIZE="4" COLOR="navy">Обобщение (для
нейронных сетей).</font></a> Способность <a HREF="gloss_n.html#Neural Networks">нейронной сети</a> &nbsp; делать
точный прогноз на данных, не принадлежащих
исходному обучающему множеству (но взятых из
того же источника). Обычно это качество сети
достигается разбиением имеющихся данных на три
подмножества; первое из них используется для
обучения сети, второе - для кросс-проверки
алгоритма обучения во время его работы, и третье -
для окончательного независимого тестирования.<br>
<br>
<br>
<a NAME="GRNN"><font SIZE="4" COLOR="navy">Обобщенно-регрессионная
нейронная сеть (GRNN).</font></a> Вид <a HREF="gloss_n.html#Neural Networks">нейронной сети</a>, где для
регрессии используются ядерная аппроксимация.
Один из видов так называемых байесовых сетей
(Speckt, 1991; Patterson, 1996; Bishop, 1995).<br>
<br>
<br>
<a NAME="Back Propagation"><font SIZE="4" COLOR="navy">Обратное
распространение.</font></a> Алгоритм обучения <a HREF="gloss_m.html#Multilayer Perceptrons">многослойных
персептронов</a>. Надежный и хорошо известный,
однако существенно более медленный по сравнению
с некоторыми современными алгоритмами (Patterson, 1996;
Fausett, 1994; Haykin, 1994).<br>
<br>
<br>
<a NAME="Communality"><font SIZE="4" COLOR="navy">Общность.</font></a> В <a HREF="../modules/stfacan.html"><i>анализе главных компонент и
факторном анализе</i></a> общность - это доля
дисперсии, которая является общей для данной и
всех остальных переменных. Доля дисперсии,
которая является характерной для данной
переменной (иногда называется характерностью)
получается после вычитанием общности из
дисперсии переменной. Другими словами дисперсия
переменной есть общность плюс характерность.
Обычно вначале в качестве оценки общности
используют коэффициент множественной
корреляции выбранной переменной со всеми
другими (для получения сведений о теории
множественной регрессии обратитесь к разделу <a HREF="../modules/stmulreg.html"><i>Множественная регрессия</i></a>).
Некоторые авторы предлагают различные
итеративные &quot;улучшения&quot; начальной оценки
общности, полученной с использованием
множественной регрессии; например, так
называемый метод MINRES (метод минимальных
факторных остатков; Харман и Джонс (Harman, Jones, 1966))
производит проверку различных модификаций
факторных нагрузок с целью минимизации
остаточных (необъясненных) сумм квадратов. <br>
<br>
<br>
<a NAME="Explained Variance"><font SIZE="4" COLOR="navy">Объясненная
дисперсия.</font></a> Доля вариации данных,
учитываемая моделью (например, <a HREF="gloss_m.html#Multiple Regression">множественной регрессии</a>,
<a HREF="gloss_d.html#General ANOVA">дисперсионного анализа</a>, <a HREF="gloss_n.html#Nonlinear Estimation">нелинейного оценивания</a>
или <a HREF="gloss_n.html#Neural Networks">нейронной сети</a>).<br>
<br>
<br>
<a NAME="Assignable Causes and Actions"><font SIZE="4" COLOR="navy">Объясняемые
причины и действия.</font></a> В ходе наблюдения за
характеристиками качества процесса следует
различать два типа изменчивости. Изменчивость,
вызванная общими причинами, соответствует
случайной динамике, свойственной этому процессу
и свойственной всем наблюдаемым значениям. В
идеальном случае, если процесс находится в
допустимых пределах, должна присутствовать
только обычная изменчивость. На карте контроля
качества эта изменчивость изображается в виде
случайных колебаний точек, соответствующих
отдельным выборкам, вокруг центральной линии,
при которых все выборки находятся между верхней
и нижней контрольной линиями и отсутствуют
какие-либо неслучайные конфигурации наблюдений
из соседних выборок. Изменчивость, вызванная
специальными или объясняемыми причинами, может
быть объяснена наличием особых обстоятельств.
Обычно на карте контроля качества это
проявляется в виде сильно уклоняющихся выборок
(т.е. превышающих нижний или верхний контрольный
пределы) или в виде систематической конфигурации
(серии) из соседних выборок. Эта изменчивость
также влияет на вычисление некоторых
характеристик карты (центральной линии и
контрольных пределов).</p>

<p>Если вы изучите условия соответствующие
недопустимости процесса и обнаружите объяснение
для их возникновения, вы сможете сопоставить
этим недопустимым выборкам осмысленные описания
и объяснить причины (например, дефект некоторого
клапана), и произведенные действия, которые их
вызвали (например, починка клапана). Наличие на
карте причин и действий показывает, что
центральная линия и контрольные пределы были
подвержены влиянию изменчивости процесса из-за
особых причин. <br>
<br>
<br>
<a NAME="Bartlett Window"><font SIZE="4" COLOR="navy">Окно Бартлетта.</font></a>
В&nbsp; анализе&nbsp; <a HREF="../modules/sttimser.html">временных
рядов</a> окно Бартлетта означает сглаживание
значений периодограммы взвешенным скользящим
средним. В окне Бартлетта (Бартлетт, 1950) веса
вычисляются как:</p>

<p><font color="blue">w<sub>j</sub> = 1-(j/p)&nbsp;&nbsp;&nbsp;&nbsp;(для j от 0 до
p)<br>
w<sub>-j</sub> = w<sub>j</sub>&nbsp;&nbsp;&nbsp;&nbsp;(для j <img src="../graphics/netblue.gif" align="absmiddle" WIDTH="11" HEIGHT="12"> 0) </font></p>

<p>где <font color="blue"><i>p = (m-1)/2</i></font> </p>

<p>Эта весовая функция приписывает больший вес
сглаживаемому наблюдению, находящемуся в центре
окна и меньшие веса значениям по мере удаления от
центра. </p>

<p>См. также раздел <a HREF="../modules/sttimser.html#spectrum">Основные
понятия и принципы</a>.<br>
<br>
<br>
<a NAME="Daniell (or Equal Weight) Window"><font SIZE="4" COLOR="navy">Окно
Даниэля (равные веса).</font></a> В анализе&nbsp; <a HREF="../modules/sttimser.html">временных рядов</a> окно Даниэля
(Daniell, 1946) означает простое (с равными весами)
сглаживание значений периодограммы скользящим
средним; т.е. каждая оценка спектральной
плотности вычисляется как среднее m/2 предыдущих
и последующих значений периодограммы. </p>

<p>См. также раздел <a HREF="../modules/sttimser.html#spectrum">Основные
понятия и принципы</a>.<br>
<br>
<br>
<a NAME="Parzen Window"><font SIZE="4" COLOR="navy">Окно Парзена.</font></a> В
анализе&nbsp; <a HREF="../modules/sttimser.html">временных рядов</a>
окно Парзена означает сглаживание значений
периодограммы взвешенным скользящим средним. В
окне Парзена (Парзен, 1961), для каждой частоты, веса
для взвешенного скользящего среднего значений
периодограммы вычисляются как:</p>

<p><font color="blue">w<sub>j</sub> = 1-6*(j/p)<sup>2</sup> + 6*(j/p)<sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;(для
j от 0 до p/2)<br>
w<sub>j</sub> = 2*(1-j/p)<sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;(для j от p/2 + 1 до
p)<br>
w<sub>-j</sub> = w<sub>j</sub>&nbsp;&nbsp;&nbsp;&nbsp;(для j <img src="../graphics/netblue.gif" align="absmiddle" WIDTH="11" HEIGHT="12"> 0) </font></p>

<p>где <font color="blue"><i>p = (m-1)/2</i></font> </p>

<p>Эта весовая функция приписывает больший вес
сглаживаемому наблюдению, находящемуся в центре
окна и меньшие веса значениям по мере удаления от
центра.</p>

<p>См. также раздел <a HREF="../modules/sttimser.html#spectrum">Основные
понятия и принципы</a>.</p>

<p>В <a HREF="gloss_n.html#Neural Networks">нейронных сетях</a> окно
Парзена - это альтернативное название метода
ядерной аппроксимации, см. например, разделы о
&nbsp; <a HREF="gloss_v.html#PNN">вероятностных</a>&nbsp; и&nbsp; <a HREF="gloss_o.html#GRNN">обобщенно-регрессионных нейронных
сетях</a> (Parzen, 1962).<br>
<br>
<br>
<a NAME="Tukey Window"><font SIZE="4" COLOR="navy">Окно Тьюки.</font></a> В
анализе&nbsp; <a HREF="../modules/sttimser.html">временных рядов</a>
окно Тьюки означает сглаживание значений
периодограммы взвешенным скользящим средним. В
окне Тьюки (Блэкмэн и Тьюки, 1958) или Тьюки-Ханна
(названное в честь Джулиуса Ван Ханна), для каждой
частоты, веса для взвешенного скользящего
среднего значений периодограммы вычисляются
как:</p>

<p><font color="blue">w<sub>j</sub> = 0.5 + 0.5*cos(<img src="../graphics/piblue.gif" align="absmiddle" WIDTH="15" HEIGHT="17">*j/p)&nbsp;&nbsp;&nbsp;&nbsp;(для j от 0 до p)<br>
w<sub>-j</sub> = w<sub>j</sub>&nbsp;&nbsp;&nbsp;&nbsp;(для j <img src="../graphics/netblue.gif" align="absmiddle" WIDTH="11" HEIGHT="12"> 0) </font></p>

<p>где <font color="blue"><i>p</i> = <i>(m-1)/2x</i></font>. </p>

<p>Эта весовая функция приписывает больший вес
сглаживаемому наблюдению, находящемуся в центре
окна и меньшие веса значениям по мере удаления от
центра.</p>

<p>См. также раздел <a HREF="../modules/sttimser.html#spectrum">Основные
понятия и принципы</a>.<br>
<br>
<br>
<a NAME="Hamming Window"><font SIZE="4" COLOR="navy">Окно Хемминга.</font></a>
В анализе&nbsp; <a HREF="../modules/sttimser.html">временных рядов</a>
окно Хемминга означает сглаживание значений
периодограммы взвешенным скользящим средним. В
окне Хемминга (названного в честь Р. В. Хемминга)
или Тьюки-Хемминга (Блэкмэн и Тьюки, 1958), для
каждой частоты, веса для взвешенного скользящего
среднего значений периодограммы вычисляются
как:</p>

<p><font color="blue">w<sub>j</sub> = 0.54 + 0.46*cosine(<img src="../graphics/piblue.gif" align="absmiddle" WIDTH="15" HEIGHT="17">*j/p)&nbsp;&nbsp;&nbsp;&nbsp;(для j от 0 до p)<br>
w<sub>-j</sub> = w<sub>j</sub>&nbsp;&nbsp;&nbsp;&nbsp;(для j <img src="../graphics/netblue.gif" align="absmiddle" WIDTH="11" HEIGHT="12"> 0) </font></p>

<p>где <font color="blue"><i>p = (m-1)/2</i></font> </p>

<p>Эта весовая функция приписывает больший вес
сглаживаемому наблюдению, находящемуся в центре
окна и меньшие веса значениям по мере удаления от
центра. </p>

<p>См. также раздел <a HREF="../modules/sttimser.html#spectrum">Основные
понятия и принципы</a>.</p>

<p><br>
<a NAME="Neighborhood"><font SIZE="4" COLOR="navy">Окрестность (для
нейронных сетей).</font></a> В <a HREF="gloss_k.html#Kohonen Training">обучении
по Кохонену</a> &nbsp; - квадрат, составленный из
элементов сети, окружающих &quot;выигравший&quot;
элемент, которые одновременно корректируются
при обучении.<br>
<br>
<br>
<a NAME="Olap"><font SIZE="4" COLOR="navy">Оперативная
аналитическая обработка данных (OLAP) (Быстрый
анализ распределенной многомерной информации -
FASMI).</font></a> Термин <em>оперативная аналитическая
обработка данных</em> обозначает технологию,
позволяющую пользователю, работающему с
многомерными базами данных, в реальном времени
получать описательные или сравнительные сводки
(&quot;срезы&quot; - &quot;views&quot;) данных и ответы на
другие аналитические запросы.</p>

<p>Дополнительную информацию см. в разделах <a HREF="../modules/stdatmin.html#olap">Оперативная аналитическая
обработка данных (OLAP)</a>, а также <a HREF="../modules/stdatmin.html#warehousing">Хранилища данных</a> и <a HREF="../modules/stdatmin.html#mining">Методы добычи данных</a>. <br>
<br>
<br>
<a NAME="Residual"><font SIZE="4" COLOR="navy">Остатки.</font></a> Остатки -
это разности между наблюдаемыми значениями и
значениями, предсказанными изучаемой моделью.
Чем лучше модель согласуется с данными, тем
меньше величина остатков. <em>i</em>-ый остаток (<i>e<sub>i</sub></i>)
вычисляется как:</p>

<p><font color="blue">e<sub>i</sub> = (y<sub>i</sub> - y<sub>i</sub>-с крышкой) </font></p>

<p>где<br>
<font color="blue"><i>y<sub>i</sub></i></font>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
- наблюдаемое значение<br>
<font color="blue"><i>y<sub>i</sub>-с крышкой</i></font>&nbsp;&nbsp;&nbsp;-
соответствующее предсказанное значение.</p>

<p>См. раздел&nbsp; <a HREF="../modules/stmulreg.html">Множественная
регрессия</a>, а также разделы о <a HREF="gloss_s.html#Standard residual value">стандартизованных
остатках</a>, <a HREF="gloss_r.html#Mahalanobis distance">расстояниях
Махаланобиса</a>, <a HREF="gloss_u.html#Deleted residual">удаленных
остатках</a>&nbsp; и&nbsp; <a HREF="gloss_r.html#Cooks distance">расстояниях
Кука</a>. <br>
<br>
<br>
<a NAME="One-Off"><font SIZE="4" COLOR="navy">Отдельное наблюдение
(для нейронных сетей).</font></a> Наблюдение, данные
которого вводятся с клавиатуры и которое затем
подается на вход <a HREF="gloss_n.html#Neural Networks">нейронной
сети</a> отдельно (а не как часть какого-то файла
данных; в обучении такие наблюдения не
участвуют). См. раздел&nbsp; <a HREF="../modules/stneunet.html">Нейронные
сети</a>.<br>
<br>
<br>
<a NAME="Response Surface"><font SIZE="4" COLOR="navy">Отклика
поверхность.</font></a> Поверхность, изображенная в
трехмерном пространстве, представляющая отклик
одной или нескольких переменных (в <a HREF="gloss_n.html#Neural Networks">нейронной сети</a>) в
зависимости от двух входных переменных при
постоянных остальных. См. разделы&nbsp; <a HREF="../modules/stexdes.html">Планирование эксперимента</a>, <a HREF="../modules/stneunet.html">Нейронные сети</a>.<br>
<br>
<br>
<a NAME="Deviation"><font SIZE="4" COLOR="navy">Отклонение.</font></a> В
радиальных элементах - величина, на которую
умножается квадрат расстояния от элемента до
входного вектора, в результате чего получается
уровень активации элемента, который затем
пропускается через <a HREF="gloss_f.html#Activation Function">функцию
активации</a>. См. раздел&nbsp; <a HREF="../modules/stneunet.html">Нейронные
сети</a>.<br>
<br>
<br>
<a NAME="SD Ratio"><font SIZE="4" COLOR="navy">Отношение стандартных
отклонений.</font></a> В задачах <a HREF="gloss_r.html#Regression">регрессии</a>
- отношение стандартного отклонения ошибки
прогноза к стандартному отклонению исходных
выходных данных. Чем меньше отношение, тем выше
точность прогноза. Эта величина равна единице
минус объясненная доля дисперсии модели. См. <a HREF="../modules/stmulreg.html">Множественная регрессия</a>, <a HREF="../modules/stneunet.html">Нейронные сети</a>.<br>
<br>
<br>
<a NAME="Negative Correlation"><font SIZE="4" COLOR="navy">Отрицательная
корреляция.</font></a> Две переменные могут быть
связаны таким образом, что при возрастании
значений одной из них значения другой убывают.
Это и показывает отрицательный коэффициент
корреляции. Про такие переменные говорят, что они
отрицательно коррелированы.</p>

<p>См. также <a HREF="../modules/stbasic.html#Correlations">Корреляции -
Вводный обзор</a>.<br>
<br>
<br>
<a NAME="Negative Exponential (2D graphs)"><font SIZE="4" COLOR="navy">Отрицательная
экспоненциально взвешенная подгонка (2М графики).</font></a>
В соответствии с процедурой отрицательного
экспоненциально взвешенного сглаживания кривая
подгоняется к координатам XY данных таким
образом, что влияние отдельных точек уменьшается
с увеличением расстояния по горизонтали от
соответствующих точек на кривой. <br>
<br>
<br>
<a NAME="Negative Exponential (3D graphs)"><font SIZE="4" COLOR="navy">Отрицательная
экспоненциально-взвешенная подгонка (3М графики).</font></a>
В соответствии с процедурой отрицательного
экспоненциально-взвешенного сглаживания
поверхность подгоняется к координатам XYZ данных
таким образом, что влияние отдельных точек
экспоненциально убывает с увеличением
расстояния по горизонтали от соответствующих
точек на поверхности. <br>
</p>

<p><a NAME="Pruning"><font SIZE="4" COLOR="navy">Отсечение (для
деревьев классификации).</font></a> <em>Отсечение</em> в <a HREF="gloss_d.html#Classification Trees">дереве классификации</a>
связано с использованием процедур
автоматического выбора дерева &quot;подходящего
размера&quot;, разработанных Breiman et. al. (1984). Эти
процедуры предназначены для того, чтобы, не
обращаясь к субъективным оценкам, находить
дерево классификации &quot;подходящего размера&quot;,
то есть дерево классификации с нужным числом
ветвлений и оптимальной точностью прогноза.
Процесс нахождения дерева классификации
&quot;подходящего размера&quot; описан в разделе <a HREF="../modules/stclatre.html#computation4">Вычислительные методы</a>&nbsp;
в главе о деревьях классификации. <br>
</p>

<P><a NAME="Type I Error Rate"><font SIZE="4" COLOR="navy">Ошибка I рода (Alpha).</font></a>
Вероятность отвержения нулевой статистической гипотезы в случае, когда она верна.</p>
<p>См. также раздел <a href="../modules/stpowan.html">Анализ мощности</a>.</p>

</body>
</html>
